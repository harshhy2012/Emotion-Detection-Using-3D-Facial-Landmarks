{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "7df332387c0946b9fa392827f6610f004e6a54f5679a7c41b4cedf79853f2c0c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path =  os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
    "path.replace('/','\\\\')\n",
    "path += '\\\\'\n",
    "\n",
    "file = ['Emotion_final_CK_plus_XYZ_NORMAL.csv',\n",
    "        'Emotion_final_CK_plus_XXX_NORMAL.csv',\n",
    "        'Emotion_final_CK_plus_SPH_NORMAL.csv',\n",
    "        'Emotion_final_CK_plus_SPH_NORMAL_XXX.csv',\n",
    "        'Emotion_final_CK_plus_ANGLE.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'d:\\\\majorProjectV2\\\\Emotion_final_CK_plus_SPH_NORMAL_XXX.csv'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "path = path+file[3]\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "905 -2.149873 -2.066373 -1.932773 -1.865974 -1.715674 -1.364975 -1.014276   \n",
       "906 -2.070828 -1.922111 -1.773394 -1.699036 -1.494550 -1.197116 -0.918272   \n",
       "907 -2.003683 -1.918866 -1.851012 -1.715304 -1.562633 -1.342109 -1.053730   \n",
       "908 -2.029320 -2.029320 -1.880370 -1.814170 -1.665220 -1.433521 -1.052871   \n",
       "909 -2.064208 -1.984646 -1.924975 -1.845414 -1.706182 -1.427717 -1.089581   \n",
       "910 -2.033716 -1.965842 -1.830095 -1.745253 -1.609505 -1.321042 -0.964705   \n",
       "911 -2.175125 -2.091514 -1.966097 -1.819777 -1.673457 -1.318109 -0.983663   \n",
       "912 -1.975732 -1.908406 -1.773753 -1.639100 -1.504448 -1.218310 -0.949005   \n",
       "913 -2.039002 -1.885096 -1.748291 -1.662788 -1.457580 -1.235271 -0.944560   \n",
       "914 -2.160379 -2.095911 -1.950859 -1.886391 -1.676871 -1.322299 -1.048311   \n",
       "915 -2.196173 -2.050963 -1.986426 -1.841216 -1.696007 -1.357185 -1.002229   \n",
       "916 -2.065232 -1.922350 -1.850908 -1.708026 -1.565143 -1.350819 -0.993612   \n",
       "917 -1.878904 -1.810889 -1.742875 -1.606845 -1.521827 -1.317783 -1.028721   \n",
       "918 -2.015910 -1.945294 -1.874679 -1.733449 -1.592218 -1.380372 -1.044950   \n",
       "919 -2.055949 -1.992810 -1.850749 -1.708687 -1.566625 -1.282502 -0.935240   \n",
       "\n",
       "            7         8         9  ...       195       196       197  \\\n",
       "905 -0.580078  0.054520  0.689118  ...  0.305765  0.181625  0.604747   \n",
       "906 -0.472121  0.104156  0.680434  ...  0.476845  0.286742  0.576563   \n",
       "907 -0.629643  0.014968  0.591725  ...  0.197446 -0.086620  0.436542   \n",
       "908 -0.688771 -0.076422  0.519377  ...  0.587155  0.370672  0.857873   \n",
       "909 -0.671884 -0.035393  0.581207  ...  0.401027  0.207535  0.544105   \n",
       "910 -0.625336  0.019464  0.664264  ...  0.351442  0.152508  0.448997   \n",
       "911 -0.544703  0.019673  0.646759  ...  0.465415  0.284721  0.544974   \n",
       "912 -0.528215  0.027228  0.649997  ...  0.542954  0.335864  0.649546   \n",
       "913 -0.499943  0.081480  0.662902  ...  0.259426  0.077645  0.448827   \n",
       "914 -0.564803  0.063757  0.627849  ...  0.393822  0.215941  0.522674   \n",
       "915 -0.566601  0.062639  0.627342  ...  0.404549  0.233335  0.519623   \n",
       "916 -0.636406  0.006566  0.560237  ...  0.396665  0.176586  0.547161   \n",
       "917 -0.671644 -0.110524  0.535615  ...  0.391276  0.163485  0.533807   \n",
       "918 -0.621258  0.014279  0.561547  ...  0.458614  0.243400  0.539328   \n",
       "919 -0.524839  0.106546  0.674793  ...  0.347377  0.137204  0.478046   \n",
       "\n",
       "          198       199       200       201       202       203  Emotion  \n",
       "905  0.658025  0.579038  0.071590  0.374573  0.409250  0.399110        7  \n",
       "906  0.614978  0.557741  0.210583  0.542330  0.568562  0.556318        6  \n",
       "907  0.496950  0.426460 -0.154408  0.313346  0.370526  0.322778        5  \n",
       "908  0.918053  0.830731  0.269608  0.628024  0.680472  0.652762        7  \n",
       "909  0.592601  0.528811  0.129907  0.467973  0.500147  0.477778        6  \n",
       "910  0.489514  0.435019  0.090491  0.427017  0.453131  0.436537        6  \n",
       "911  0.567960  0.495565  0.107849  0.488243  0.531890  0.536440        6  \n",
       "912  0.704631  0.664415  0.360152  0.648530  0.656010  0.630163        6  \n",
       "913  0.513718  0.471272  0.114387  0.400985  0.413145  0.376351        6  \n",
       "914  0.564818  0.504851  0.131339  0.455057  0.480303  0.470557        6  \n",
       "915  0.551348  0.479267  0.078039  0.426229  0.464299  0.463024        6  \n",
       "916  0.610016  0.570087  0.214550  0.516760  0.526110  0.495349        6  \n",
       "917  0.590390  0.542684  0.165089  0.496974  0.518473  0.485905        6  \n",
       "918  0.587534  0.543632  0.224594  0.545223  0.561080  0.540073        6  \n",
       "919  0.534549  0.488786  0.145508  0.460516  0.476914  0.448565        6  \n",
       "\n",
       "[15 rows x 205 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>195</th>\n      <th>196</th>\n      <th>197</th>\n      <th>198</th>\n      <th>199</th>\n      <th>200</th>\n      <th>201</th>\n      <th>202</th>\n      <th>203</th>\n      <th>Emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>905</th>\n      <td>-2.149873</td>\n      <td>-2.066373</td>\n      <td>-1.932773</td>\n      <td>-1.865974</td>\n      <td>-1.715674</td>\n      <td>-1.364975</td>\n      <td>-1.014276</td>\n      <td>-0.580078</td>\n      <td>0.054520</td>\n      <td>0.689118</td>\n      <td>...</td>\n      <td>0.305765</td>\n      <td>0.181625</td>\n      <td>0.604747</td>\n      <td>0.658025</td>\n      <td>0.579038</td>\n      <td>0.071590</td>\n      <td>0.374573</td>\n      <td>0.409250</td>\n      <td>0.399110</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>906</th>\n      <td>-2.070828</td>\n      <td>-1.922111</td>\n      <td>-1.773394</td>\n      <td>-1.699036</td>\n      <td>-1.494550</td>\n      <td>-1.197116</td>\n      <td>-0.918272</td>\n      <td>-0.472121</td>\n      <td>0.104156</td>\n      <td>0.680434</td>\n      <td>...</td>\n      <td>0.476845</td>\n      <td>0.286742</td>\n      <td>0.576563</td>\n      <td>0.614978</td>\n      <td>0.557741</td>\n      <td>0.210583</td>\n      <td>0.542330</td>\n      <td>0.568562</td>\n      <td>0.556318</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>907</th>\n      <td>-2.003683</td>\n      <td>-1.918866</td>\n      <td>-1.851012</td>\n      <td>-1.715304</td>\n      <td>-1.562633</td>\n      <td>-1.342109</td>\n      <td>-1.053730</td>\n      <td>-0.629643</td>\n      <td>0.014968</td>\n      <td>0.591725</td>\n      <td>...</td>\n      <td>0.197446</td>\n      <td>-0.086620</td>\n      <td>0.436542</td>\n      <td>0.496950</td>\n      <td>0.426460</td>\n      <td>-0.154408</td>\n      <td>0.313346</td>\n      <td>0.370526</td>\n      <td>0.322778</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>908</th>\n      <td>-2.029320</td>\n      <td>-2.029320</td>\n      <td>-1.880370</td>\n      <td>-1.814170</td>\n      <td>-1.665220</td>\n      <td>-1.433521</td>\n      <td>-1.052871</td>\n      <td>-0.688771</td>\n      <td>-0.076422</td>\n      <td>0.519377</td>\n      <td>...</td>\n      <td>0.587155</td>\n      <td>0.370672</td>\n      <td>0.857873</td>\n      <td>0.918053</td>\n      <td>0.830731</td>\n      <td>0.269608</td>\n      <td>0.628024</td>\n      <td>0.680472</td>\n      <td>0.652762</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>909</th>\n      <td>-2.064208</td>\n      <td>-1.984646</td>\n      <td>-1.924975</td>\n      <td>-1.845414</td>\n      <td>-1.706182</td>\n      <td>-1.427717</td>\n      <td>-1.089581</td>\n      <td>-0.671884</td>\n      <td>-0.035393</td>\n      <td>0.581207</td>\n      <td>...</td>\n      <td>0.401027</td>\n      <td>0.207535</td>\n      <td>0.544105</td>\n      <td>0.592601</td>\n      <td>0.528811</td>\n      <td>0.129907</td>\n      <td>0.467973</td>\n      <td>0.500147</td>\n      <td>0.477778</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>910</th>\n      <td>-2.033716</td>\n      <td>-1.965842</td>\n      <td>-1.830095</td>\n      <td>-1.745253</td>\n      <td>-1.609505</td>\n      <td>-1.321042</td>\n      <td>-0.964705</td>\n      <td>-0.625336</td>\n      <td>0.019464</td>\n      <td>0.664264</td>\n      <td>...</td>\n      <td>0.351442</td>\n      <td>0.152508</td>\n      <td>0.448997</td>\n      <td>0.489514</td>\n      <td>0.435019</td>\n      <td>0.090491</td>\n      <td>0.427017</td>\n      <td>0.453131</td>\n      <td>0.436537</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>911</th>\n      <td>-2.175125</td>\n      <td>-2.091514</td>\n      <td>-1.966097</td>\n      <td>-1.819777</td>\n      <td>-1.673457</td>\n      <td>-1.318109</td>\n      <td>-0.983663</td>\n      <td>-0.544703</td>\n      <td>0.019673</td>\n      <td>0.646759</td>\n      <td>...</td>\n      <td>0.465415</td>\n      <td>0.284721</td>\n      <td>0.544974</td>\n      <td>0.567960</td>\n      <td>0.495565</td>\n      <td>0.107849</td>\n      <td>0.488243</td>\n      <td>0.531890</td>\n      <td>0.536440</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>912</th>\n      <td>-1.975732</td>\n      <td>-1.908406</td>\n      <td>-1.773753</td>\n      <td>-1.639100</td>\n      <td>-1.504448</td>\n      <td>-1.218310</td>\n      <td>-0.949005</td>\n      <td>-0.528215</td>\n      <td>0.027228</td>\n      <td>0.649997</td>\n      <td>...</td>\n      <td>0.542954</td>\n      <td>0.335864</td>\n      <td>0.649546</td>\n      <td>0.704631</td>\n      <td>0.664415</td>\n      <td>0.360152</td>\n      <td>0.648530</td>\n      <td>0.656010</td>\n      <td>0.630163</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>913</th>\n      <td>-2.039002</td>\n      <td>-1.885096</td>\n      <td>-1.748291</td>\n      <td>-1.662788</td>\n      <td>-1.457580</td>\n      <td>-1.235271</td>\n      <td>-0.944560</td>\n      <td>-0.499943</td>\n      <td>0.081480</td>\n      <td>0.662902</td>\n      <td>...</td>\n      <td>0.259426</td>\n      <td>0.077645</td>\n      <td>0.448827</td>\n      <td>0.513718</td>\n      <td>0.471272</td>\n      <td>0.114387</td>\n      <td>0.400985</td>\n      <td>0.413145</td>\n      <td>0.376351</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>914</th>\n      <td>-2.160379</td>\n      <td>-2.095911</td>\n      <td>-1.950859</td>\n      <td>-1.886391</td>\n      <td>-1.676871</td>\n      <td>-1.322299</td>\n      <td>-1.048311</td>\n      <td>-0.564803</td>\n      <td>0.063757</td>\n      <td>0.627849</td>\n      <td>...</td>\n      <td>0.393822</td>\n      <td>0.215941</td>\n      <td>0.522674</td>\n      <td>0.564818</td>\n      <td>0.504851</td>\n      <td>0.131339</td>\n      <td>0.455057</td>\n      <td>0.480303</td>\n      <td>0.470557</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>915</th>\n      <td>-2.196173</td>\n      <td>-2.050963</td>\n      <td>-1.986426</td>\n      <td>-1.841216</td>\n      <td>-1.696007</td>\n      <td>-1.357185</td>\n      <td>-1.002229</td>\n      <td>-0.566601</td>\n      <td>0.062639</td>\n      <td>0.627342</td>\n      <td>...</td>\n      <td>0.404549</td>\n      <td>0.233335</td>\n      <td>0.519623</td>\n      <td>0.551348</td>\n      <td>0.479267</td>\n      <td>0.078039</td>\n      <td>0.426229</td>\n      <td>0.464299</td>\n      <td>0.463024</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>916</th>\n      <td>-2.065232</td>\n      <td>-1.922350</td>\n      <td>-1.850908</td>\n      <td>-1.708026</td>\n      <td>-1.565143</td>\n      <td>-1.350819</td>\n      <td>-0.993612</td>\n      <td>-0.636406</td>\n      <td>0.006566</td>\n      <td>0.560237</td>\n      <td>...</td>\n      <td>0.396665</td>\n      <td>0.176586</td>\n      <td>0.547161</td>\n      <td>0.610016</td>\n      <td>0.570087</td>\n      <td>0.214550</td>\n      <td>0.516760</td>\n      <td>0.526110</td>\n      <td>0.495349</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>917</th>\n      <td>-1.878904</td>\n      <td>-1.810889</td>\n      <td>-1.742875</td>\n      <td>-1.606845</td>\n      <td>-1.521827</td>\n      <td>-1.317783</td>\n      <td>-1.028721</td>\n      <td>-0.671644</td>\n      <td>-0.110524</td>\n      <td>0.535615</td>\n      <td>...</td>\n      <td>0.391276</td>\n      <td>0.163485</td>\n      <td>0.533807</td>\n      <td>0.590390</td>\n      <td>0.542684</td>\n      <td>0.165089</td>\n      <td>0.496974</td>\n      <td>0.518473</td>\n      <td>0.485905</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>918</th>\n      <td>-2.015910</td>\n      <td>-1.945294</td>\n      <td>-1.874679</td>\n      <td>-1.733449</td>\n      <td>-1.592218</td>\n      <td>-1.380372</td>\n      <td>-1.044950</td>\n      <td>-0.621258</td>\n      <td>0.014279</td>\n      <td>0.561547</td>\n      <td>...</td>\n      <td>0.458614</td>\n      <td>0.243400</td>\n      <td>0.539328</td>\n      <td>0.587534</td>\n      <td>0.543632</td>\n      <td>0.224594</td>\n      <td>0.545223</td>\n      <td>0.561080</td>\n      <td>0.540073</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>919</th>\n      <td>-2.055949</td>\n      <td>-1.992810</td>\n      <td>-1.850749</td>\n      <td>-1.708687</td>\n      <td>-1.566625</td>\n      <td>-1.282502</td>\n      <td>-0.935240</td>\n      <td>-0.524839</td>\n      <td>0.106546</td>\n      <td>0.674793</td>\n      <td>...</td>\n      <td>0.347377</td>\n      <td>0.137204</td>\n      <td>0.478046</td>\n      <td>0.534549</td>\n      <td>0.488786</td>\n      <td>0.145508</td>\n      <td>0.460516</td>\n      <td>0.476914</td>\n      <td>0.448565</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>15 rows × 205 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "df = pd.read_csv(path)\n",
    "df = df.drop(['Image_name'], axis = 1)\n",
    "df = df.drop(['Unnamed: 0'], axis = 1)\n",
    "for i in range(1,10):\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Emotion', axis=1)\n",
    "y = df.Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "dtree_model = DecisionTreeClassifier(max_depth = 2).fit(X_train, y_train) \n",
    "dtree_predictions = dtree_model.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         8\n           1       0.00      0.00      0.00        14\n           2       0.00      0.00      0.00         4\n           3       0.00      0.00      0.00        16\n           4       0.00      0.00      0.00        11\n           5       0.60      0.95      0.73        19\n           6       0.77      0.99      0.87       172\n           7       1.00      0.78      0.88        32\n\n    accuracy                           0.77       276\n   macro avg       0.30      0.34      0.31       276\nweighted avg       0.64      0.77      0.69       276\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, dtree_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7748062015503876"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "scores = cross_val_score(dtree_model,X_train, y_train,cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}